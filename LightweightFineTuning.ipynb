{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "Description of Choices\n",
    "\n",
    "* PEFT technique: LoRA (Low-Rank Adaptation) is the selected PEFT techniques since it is compatible with all models\n",
    "* Model: GPT-2 is the selected model since it is relatively small and compatible with sequence classification and LoRA\n",
    "* Evaluation approach: Evaluation will be conducted using Hugging Face's `Trainer` class which simplifies the training and evaluation workflow. accuracy and F1-score are computed using `compute_metrics` function which allaows for a comparison of the performance of the original model against the fine-tunned model\n",
    "* Fine-tuning dataset: The IMDb dataset from the Hugging Face `datasets` library will be used for fine-tunning. This dataset is a standard benchmark for binary sentiment classification tasks making it a good fit in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating Foundation Model\n",
    "\n",
    "Load GPT-2 pre-trained Hugging Face model and evaluate its performance prior to fine-tuning\n",
    "\n",
    "The following steps are taken:\n",
    "- Load the GPT-2 model and tokenizer.\n",
    "- Add a padding token to the tokenizer for compatibility.\n",
    "- Preprocess the IMDb dataset for sequence classification.\n",
    "- Evaluate the baseline performance using accuracy and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "589c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 07:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Performance: {'eval_loss': 0.8299095630645752, 'eval_model_preparation_time': 0.0007, 'eval_accuracy': 0.51, 'eval_f1': 0.35017791538992193, 'eval_runtime': 62.6302, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 7.983}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1})\n",
    "\n",
    "# Add padding token to the tokenizer if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000)) \n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "baseline_metrics = trainer.evaluate()\n",
    "print(\"Baseline Performance:\", baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3deac38",
   "metadata": {},
   "source": [
    "#### Key Observation ####\n",
    "The metrics suggest that while the model has decent initialization and speed, its performance (accuracy and F1 score) leaves significant room for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.\n",
    "\n",
    "In this section:\n",
    "- A LoRA configuration is created with specified parameters for adaptation.\n",
    "- The PEFT model is trained and fine-tuned parameters are saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 20:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.241500</td>\n",
       "      <td>0.918445</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.829076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.399800</td>\n",
       "      <td>0.845975</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.875672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.816857</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.879731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", ## SEQ_CLS for sequence classification tasks\n",
    "    inference_mode=False, \n",
    "    lora_dropout=0.2\n",
    ")\n",
    "\n",
    "# Create PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Update the Trainer to use the PEFT model\n",
    "trainer.model = peft_model\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the PEFT model weights\n",
    "peft_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,448 || all params: 124,737,792 || trainable%: 0.2377\n"
     ]
    }
   ],
   "source": [
    "# check trainable parameters of model\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bb83e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Base model metrics: {'eval_loss': 0.8299095630645752, 'eval_model_preparation_time': 0.0007, 'eval_accuracy': 0.51, 'eval_f1': 0.35017791538992193, 'eval_runtime': 62.6302, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 7.983}\n",
      "PEFT model metrics: {'eval_loss': 0.8168573975563049, 'eval_model_preparation_time': 0.0022, 'eval_accuracy': 0.88, 'eval_f1': 0.8797307692307693, 'eval_runtime': 66.1406, 'eval_samples_per_second': 7.56, 'eval_steps_per_second': 7.56, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "peft_metrics = trainer.evaluate()\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Base model metrics:\", baseline_metrics)\n",
    "print(\"PEFT model metrics:\", peft_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ce733",
   "metadata": {},
   "source": [
    "#### Key Observations ####\n",
    "- The PEFT model significantly outperforms the Base Model in accuracy and F1 score, indicating better generalization and effectiveness.\n",
    "- The PEFT model slightly reduces the evaluation loss compared to the Base Model.\n",
    "- The PEFT model requires slightly more preparation time, which is negligible compared to the performance gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Load the saved PEFT model weights and evaluate the performance of the trained PEFT model. The results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModelForSequenceClassification\n",
    "\n",
    "# Reload the PEFT model\n",
    "fine_tuned_model = PeftModelForSequenceClassification.from_pretrained(model,\"gpt-lora\")\n",
    "\n",
    "# Tokenize sample texts\n",
    "sample_texts = [\n",
    "    \"The movie was absolutely wonderful! A masterpiece.\",\n",
    "    \"Terrible movie. I would not recommend it to anyone.\",\n",
    "    \"It was just okay, nothing too special.\",\n",
    "]\n",
    "inputs = tokenizer(sample_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Move model and inputs to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "fine_tuned_model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Perform inference\n",
    "outputs = fine_tuned_model(**inputs)\n",
    "predictions = np.argmax(outputs.logits.detach().numpy(), axis=-1)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3f41b",
   "metadata": {},
   "source": [
    "### Key Observations ###\n",
    "\n",
    "- The model accurately classified one review as positive (1), which was a highly favorable comment about the movie.\n",
    "- It correctly identified a negative review as negative (0), showcasing its ability to discern critical feedback.\n",
    "- A neutral or mixed review was also classified as negative (0), which might indicate a lack of nuance in distinguishing between neutral and negative sentiments.\n",
    "\n",
    "Overall, the model performed well in identifying clear positive and negative sentiments but might need refinement to handle more nuanced or neutral statements effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
