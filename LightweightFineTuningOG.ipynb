{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "Description of Choices\n",
    "\n",
    "* PEFT technique: LoRA (Low-Rank Adaptation) is the selected PEFT techniques since it is compatible with all models\n",
    "* Model: GPT-2 is the selected model since it is relatively small and compatible with sequence classification and LoRA\n",
    "* Evaluation approach: Evaluation will be conducted using Hugging Face's `Trainer` class which simplifies the training and evaluation workflow. accuracy and F1-score are computed using `compute_metrics` function which allaows for a comparison of the performance of the original model against the fine-tunned model\n",
    "* Fine-tuning dataset: The IMDb dataset from the Hugging Face `datasets` library will be used for fine-tunning. This dataset is a standard benchmark for binary sentiment classification tasks making it a good fit in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating Foundation Model\n",
    "\n",
    "Load GPT-2 pre-trained Hugging Face model and evaluate its performance prior to fine-tuning\n",
    "\n",
    "The following steps are taken:\n",
    "- Load the GPT-2 model and tokenizer.\n",
    "- Add a padding token to the tokenizer for compatibility.\n",
    "- Preprocess the IMDb dataset for sequence classification.\n",
    "- Evaluate the baseline performance using accuracy and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "589c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1})\n",
    "\n",
    "# Add padding token to the tokenizer if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000)) \n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "baseline_metrics = trainer.evaluate()\n",
    "print(\"Baseline Performance:\", baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3deac38",
   "metadata": {},
   "source": [
    "#### Key Observation ####\n",
    "The metrics suggest that while the model has decent initialization and speed, its performance (accuracy and F1 score) can be improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.\n",
    "\n",
    "In this section:\n",
    "- A LoRA configuration is created with specified parameters for adaptation.\n",
    "- The PEFT model is trained and fine-tuned parameters are saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", ## SEQ_CLS for sequence classification tasks\n",
    "    inference_mode=False, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Create PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Update the Trainer to use the PEFT model\n",
    "trainer.model = peft_model\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the PEFT model weights\n",
    "peft_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c3286",
   "metadata": {},
   "source": [
    "#### Key Observations ####\n",
    "- The model showed strong improvement in performance with accuracy increasing from 51.2% to 83.6%\n",
    "- The are signs of overfitting in epoch 3 with training loss dropped to 0.0001\n",
    "- Accuracy and F1 scores stabilized by Epoch 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Compare the  results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModelForSequenceClassification\n",
    "\n",
    "# Reload the PEFT model\n",
    "fine_tuned_model = PeftModelForSequenceClassification.from_pretrained(model,\"gpt-lora\")\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Create trainer for evaluation\n",
    "fine_tuned_trainer = Trainer(\n",
    "    model=fine_tuned_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "fine_tuned_metrics = fine_tuned_trainer.evaluate()\n",
    "\n",
    "# Print comparison of metrics\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Metric       | Baseline | Fine-tuned\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Accuracy     | {baseline_metrics['eval_accuracy']:.4f}   | {fine_tuned_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score     | {baseline_metrics['eval_f1']:.4f}   | {fine_tuned_metrics['eval_f1']:.4f}\")\n",
    "print(f\"Loss         | {baseline_metrics['eval_loss']:.4f}   | {fine_tuned_metrics['eval_loss']:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ce733",
   "metadata": {},
   "source": [
    "#### Key Observations ####\n",
    "- The PEFT model significantly outperforms the Base Model in accuracy and F1 score, indicating better generalization and effectiveness.\n",
    "- The PEFT model slightly reduces the evaluation loss compared to the Base Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c86fbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate inference on sample texts\n",
    "sample_texts = [\n",
    "    \"The movie was absolutely wonderful! A masterpiece.\",\n",
    "    \"Terrible movie. I would not recommend it to anyone.\",\n",
    "    \"It was just okay, nothing too special.\",\n",
    "]\n",
    "inputs = tokenizer(sample_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move model and inputs to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "fine_tuned_model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Perform inference\n",
    "outputs = fine_tuned_model(**inputs)\n",
    "predictions = np.argmax(outputs.logits.detach().numpy(), axis=-1)\n",
    "\n",
    "# Make a dataframe with the sample texts, predictions, and predicted labels\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"text\": sample_texts,\n",
    "    \"prediction\": predictions,\n",
    "    \"predicted_label\": [model.config.id2label[p] for p in predictions]\n",
    "}) \n",
    "\n",
    "# Show all the cells in the dataframe\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3f41b",
   "metadata": {},
   "source": [
    "### Key Observations ###\n",
    "\n",
    "- The model accurately classified one review as positive (1), which was a highly favorable comment about the movie.\n",
    "- It correctly identified a negative review as negative (0), showcasing its ability to discern critical feedback.\n",
    "- A neutral or mixed review was also classified as negative (0), which might indicate a lack of nuance in distinguishing between neutral and negative sentiments.\n",
    "\n",
    "Overall, the model performed well in identifying clear positive and negative sentiments but might need refinement to handle more nuanced or neutral statements effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
